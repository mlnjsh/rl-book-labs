# ğŸ® RL Book Interactive Labs

**Companion interactive apps for [Complete Reinforcement Learning Journey: From Basics to RLHF]()**

> *Don't just read about algorithms â€” watch them think.*

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Pages](https://img.shields.io/badge/demo-live-brightgreen)](https://mlnjsh.github.io/rl-book-labs/)

---

## ğŸ§ª What Is This?

Each chapter of the book has a companion **browser-based interactive lab** where you can:

- ğŸ”µ **Step through algorithms** cell-by-cell and see values update in real-time
- ğŸ›ï¸ **Tweak parameters** (Î³, Îµ, learning rate) with sliders and instantly see the effect
- ğŸ¤– **Watch agents navigate** grids, solve problems, and learn from mistakes
- ğŸ“Š **Inspect Q-values**, policy arrows, and convergence logs live

**No installation required.** Open in any browser. Works on desktop and mobile.

---

## ğŸ“š Available Labs

| Chapter | Lab | Concepts Covered | Try It |
|---------|-----|-----------------|--------|
| Ch 2 | **MDP Explorer** | States, Actions, Rewards, Transitions, Deterministic vs Stochastic, Policy, Value Function | [â–¶ Launch](https://mlnjsh.github.io/rl-book-labs/ch2/) |
| Ch 3 | **Policy Iteration on FrozenLake** | Bellman equations, policy evaluation (sweep-by-sweep), policy improvement, convergence | [â–¶ Launch](https://mlnjsh.github.io/rl-book-labs/ch3/) |
| Ch 4 | Monte Carlo Blackjack *(coming soon)* | First-visit MC, exploring starts, episode replay | â€” |
| Ch 5 | TD Learning & SARSA *(coming soon)* | TD(0), SARSA, Q-learning, cliff walking | â€” |
| Ch 6 | DQN on CartPole *(coming soon)* | Experience replay, target networks, training curves | â€” |
| Ch 7 | Policy Gradients *(coming soon)* | REINFORCE, baselines, variance reduction | â€” |

---

## ğŸŒ Ch2: MDP Explorer

**Understand the building blocks of every RL algorithm.** Explore a 5Ã—5 Gridworld MDP interactively.

### Three Modes

| Mode | What You Learn |
|------|---------------|
| **ğŸ” Explore** | Click any cell â†’ see its state (r,c), reward, transition probabilities for each action, and Q-values |
| **Ï€ Policy** | See policy arrows on every cell. Click to cycle actions and build your own policy |
| **V Value** | Color-coded heatmap of V(s). Green = high value, red = low value |

### Key Features
- **Deterministic vs Stochastic** â€” slide slip from 0 to 0.6 and watch transition probabilities change
- **Click any cell** â†’ full breakdown of transitions, rewards, and Q(s,a) for all 4 actions
- **âš¡ Solve Optimal Policy** â€” finds Ï€* and shows value heatmap
- **ğŸ¤– Run Robot** â€” animated step-by-step episode
- **ğŸ¤–Ã—10 Run 10 Episodes** â€” shows success rate (deterministic vs stochastic)
- **âœï¸ Edit Grid** â€” paint walls, pits, goals, and start positions to create your own MDP

### What to Try
1. Click cells â†’ inspect State, Action, Reward, Transition
2. âš¡ Solve with **slip=0** â†’ observe shortest path
3. Set **slip=0.3** â†’ Solve again â†’ policy becomes cautious near pits!
4. Run ğŸ¤– with slip=0 â†’ always reaches goal
5. Run ğŸ¤–Ã—10 with slip=0.3 â†’ some episodes fail!
6. Compare **Î³=0.3** vs **Î³=0.99** â†’ value function changes dramatically
7. Edit grid: add more pits near the goal â†’ watch policy adapt

---

## ğŸš€ Ch3: Policy Iteration Visualizer

**Step through the Policy Iteration algorithm on a 4Ã—4 FrozenLake grid.**

### What You Can Do

| Button | What Happens |
|--------|-------------|
| **â‘  One Eval Sweep** | Each cell lights up blue as its value updates via the Bellman equation |
| **â‘  Full Evaluation** | Runs all sweeps until V^Ï€ converges |
| **â‘¡ Improve Policy** | Arrows change one-by-one to the greedy action â€” green flash on changes |
| **â–¶â–¶ Auto-Run** | Runs the full evaluate â†’ improve loop with pauses between iterations |
| **ğŸ¤– Run Robot** | Animated robot walks the grid following the current policy |
| **â†º Reset** | Start fresh with new parameters |

### What to Try
1. Press â‘  One Sweep â€” watch cells light up one by one
2. Press â‘  again â€” values get more accurate each sweep
3. Press â‘  Full Eval â€” converge V^Ï€ completely
4. Press â‘¡ â€” watch arrows change direction!
5. Repeat â‘ â†’â‘¡ until Ï€* found
6. Press ğŸ¤– â€” watch the robot navigate!
7. Try **Î³=0.5** vs **Î³=0.99** â€” compare policies
8. Try **Slip=0** vs **Slip=0.5** â€” deterministic vs stochastic

---

## ğŸ—ï¸ Project Structure

```
rl-book-labs/
â”œâ”€â”€ README.md
â”œâ”€â”€ ch2/
â”‚   â””â”€â”€ index.html          # MDP Explorer (5Ã—5 Gridworld)
â”œâ”€â”€ ch3/
â”‚   â””â”€â”€ index.html          # Policy Iteration on FrozenLake
â”œâ”€â”€ ch4/                    # (coming soon)
â”œâ”€â”€ ch5/                    # (coming soon)
â””â”€â”€ ch6/                    # (coming soon)
```

Each lab is a **single HTML file** â€” no build step, no dependencies, no frameworks. Just open in any browser.

---

## ğŸ“ About the Book

**Complete Reinforcement Learning Journey: From Basics to RLHF**

The only book that takes you from "What is a Markov Decision Process?" all the way to "How do we align language models with human values?" â€” with intuition, math, code, and interactive labs at every step.

### Key Features
- ğŸ“– **Intuition â†’ Math â†’ Code** triple for every concept
- ğŸ¤– **DeliBot** running example that grows with the theory
- ğŸ§  **Think Like an Agent** boxes for building intuition
- âš ï¸ **Common Misconceptions** boxes to prevent errors
- ğŸ”¬ **Interactive Labs** (this repo!) for hands-on learning
- ğŸ“ **Quizzes with detailed answer keys** for each chapter

---

## ğŸ¤ Contributing

Found a bug in a lab? Have an idea for a new visualization? Contributions are welcome!

1. Fork the repo
2. Create a branch (`git checkout -b feature/new-lab`)
3. Commit your changes
4. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.

---

<p align="center">
  <i>Built with â¤ï¸ as a companion to the book.</i><br>
  <i>"The best way to learn an algorithm is to watch it think."</i>
</p>
