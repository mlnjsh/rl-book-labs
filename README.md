# ğŸ® RL Book Interactive Labs

**Companion interactive apps for [Complete Reinforcement Learning Journey: From Basics to RLHF]()**

> *Don't just read about algorithms â€” watch them think.*

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Pages](https://img.shields.io/badge/demo-live-brightgreen)](https://mlnjsh.github.io/rl-book-labs/)

---

## ğŸ§ª What Is This?

Each chapter of the book has a companion **browser-based interactive lab** where you can:

- ğŸ”µ **Step through algorithms** cell-by-cell and see values update in real-time
- ğŸ›ï¸ **Tweak parameters** (Î³, Îµ, learning rate) with sliders and instantly see the effect
- ğŸ¤– **Watch agents navigate** grids, solve problems, and learn from mistakes
- ğŸ“Š **Inspect Q-values**, policy arrows, and convergence logs live

**No installation required.** Open in any browser. Works on desktop and mobile.

---

## ğŸ“š Available Labs

| Chapter | Lab | Concepts Covered | Try It |
|---------|-----|-----------------|--------|
| Ch 3 | **Policy Iteration on FrozenLake** | Bellman equations, policy evaluation (sweep-by-sweep), policy improvement, convergence | [â–¶ Launch](https://mlnjsh.github.io/rl-book-labs/ch3/) |
| Ch 4 | Monte Carlo Blackjack *(coming soon)* | First-visit MC, exploring starts, episode replay | â€” |
| Ch 5 | TD Learning & SARSA *(coming soon)* | TD(0), SARSA, Q-learning, cliff walking | â€” |
| Ch 6 | DQN on CartPole *(coming soon)* | Experience replay, target networks, training curves | â€” |
| Ch 7 | Policy Gradients *(coming soon)* | REINFORCE, baselines, variance reduction | â€” |

---

## ğŸš€ Ch3: Policy Iteration Visualizer

**The flagship lab.** Step through the Policy Iteration algorithm on a 4Ã—4 FrozenLake grid.

### What You Can Do

| Button | What Happens |
|--------|-------------|
| **â‘  One Eval Sweep** | Each cell lights up blue as its value updates via the Bellman equation |
| **â‘  Full Evaluation** | Runs all sweeps until V^Ï€ converges |
| **â‘¡ Improve Policy** | Arrows change one-by-one to the greedy action â€” green flash on changes |
| **â–¶â–¶ Auto-Run** | Runs the full evaluate â†’ improve loop with pauses between iterations |
| **ğŸ¤– Run Robot** | Animated robot walks the grid following the current policy |
| **â†º Reset** | Start fresh with new parameters |

### Parameters to Experiment With

- **Discount factor Î³** (0.1 â†’ 0.99): How much does the agent care about the future?
- **Slip probability** (0 â†’ 0.5): How stochastic is the environment?
- **Animation speed** (ğŸ¢ Slow â†’ âš¡ Fast): Control the visualization pace

### What to Try

1. Set **Î³ = 0.5** â†’ run to convergence â†’ note the policy
2. Set **Î³ = 0.99** â†’ run again â†’ compare: the agent plans further ahead!
3. Set **Slip = 0** â†’ deterministic â†’ the optimal path is obvious
4. Set **Slip = 0.5** â†’ highly stochastic â†’ the policy becomes more cautious
5. Run ğŸ¤– multiple times with high slip â†’ watch different outcomes each time

---

## ğŸ—ï¸ Project Structure

```
rl-book-labs/
â”œâ”€â”€ index.html              # Landing page with links to all labs
â”œâ”€â”€ ch3/
â”‚   â””â”€â”€ index.html          # Policy Iteration on FrozenLake (standalone)
â”œâ”€â”€ ch4/                    # (coming soon)
â”œâ”€â”€ ch5/                    # (coming soon)
â””â”€â”€ README.md
```

Each lab is a **single HTML file** â€” no build step, no dependencies, no frameworks to install. Just pure HTML + CSS + JavaScript.

---

## ğŸ“ About the Book

**Complete Reinforcement Learning Journey: From Basics to RLHF**

The only book that takes you from "What is a Markov Decision Process?" all the way to "How do we align language models with human values?" â€” with intuition, math, code, and interactive labs at every step.

### Key Features
- ğŸ“– **Intuition â†’ Math â†’ Code** triple for every concept
- ğŸ¤– **DeliBot** running example that grows with the theory
- ğŸ§  **Think Like an Agent** boxes for building intuition
- âš ï¸ **Common Misconceptions** boxes to prevent errors
- ğŸ”¬ **Interactive Labs** (this repo!) for hands-on learning
- ğŸ“ **Quizzes with detailed answer keys** for each chapter

---

## ğŸ¤ Contributing

Found a bug in a lab? Have an idea for a new visualization? Contributions are welcome!

1. Fork the repo
2. Create a branch (`git checkout -b feature/new-lab`)
3. Commit your changes
4. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.

---

<p align="center">
  <i>Built with â¤ï¸ as a companion to the book.</i><br>
  <i>"The best way to learn an algorithm is to watch it think."</i>
</p>
