# ğŸ® RL Book Interactive Labs

**Companion interactive apps & notebooks for [Complete Reinforcement Learning Journey: From Basics to RLHF]()**

> *Don't just read about algorithms â€” watch them think.*

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![GitHub Pages](https://img.shields.io/badge/demo-live-brightgreen)](https://mlnjsh.github.io/rl-book-labs/)

---

## ğŸ§ª What Is This?

Each chapter has **two companions**:
- ğŸŒ **Interactive Web App** â€” browser-based, no install, sliders + animations
- ğŸ““ **Colab Notebook** â€” full Python code, build environments, run algorithms, plot results

---

## ğŸ“š Available Labs

| Chapter | Web App | Colab Notebook | Concepts |
|---------|---------|----------------|----------|
| Ch 2: MDPs | [â–¶ MDP Explorer](https://mlnjsh.github.io/rl-book-labs/ch2/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnjsh/rl-book-labs/blob/main/Ch2_MDP_Environments_Lab.ipynb) | States, Actions, Rewards, Transitions, Deterministic vs Stochastic |
| Ch 3: DP | [â–¶ Policy Iteration](https://mlnjsh.github.io/rl-book-labs/ch3/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnjsh/rl-book-labs/blob/main/Ch3_Dynamic_Programming_Lab.ipynb) | Policy Evaluation, Policy Iteration, Value Iteration, Convergence |
| Ch 4 | *coming soon* | *coming soon* | Monte Carlo Methods |
| Ch 5 | *coming soon* | *coming soon* | TD Learning, SARSA, Q-Learning |
| Ch 6 | *coming soon* | *coming soon* | Deep RL, DQN |
| Ch 7 | *coming soon* | *coming soon* | Policy Gradients, RLHF |

---

## ğŸ““ Colab Notebooks

### Ch2: MDP Environments Lab
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnjsh/rl-book-labs/blob/main/Ch2_MDP_Environments_Lab.ipynb)

Build **7 MDP environments** from scratch in Python:

| # | Environment | States | Actions | Key Lesson |
|---|------------|--------|---------|------------|
| 1 | GridWorld 5Ã—5 | 22 cells | â†â†“â†’â†‘ | Walls, goal, pit, spatial navigation |
| 2 | FrozenLake 4Ã—4 | 16 cells | â†â†“â†’â†‘ | Slippery ice, holes |
| 3 | Traffic Light | 6 states | keep/switch | Real-world control |
| 4 | Thermostat | 3 states | heat/cool/off | Energy vs comfort tradeoff |
| 5 | Contextual Bandit | 3 contexts | machine A/B/C | Context-dependent rewards |
| 6 | Inventory Management | 5 levels | order 0/1/2 | Supply chain, stockouts |
| 7 | Robot Rooms | 4 rooms | go/stay | Locked doors, path planning |

**What you'll do:** Inspect transition tables, compute Q-values, visualize value heatmaps, compare deterministic vs stochastic, experiment with Î³.

### Ch3: Dynamic Programming Lab
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnjsh/rl-book-labs/blob/main/Ch3_Dynamic_Programming_Lab.ipynb)

Implement **3 core DP algorithms** and run them on all 7 environments:

| Algorithm | What It Does |
|-----------|-------------|
| Policy Evaluation | Compute V^Ï€ sweep by sweep â€” watch values converge |
| Policy Iteration | Evaluate â†’ Improve loop until Ï€* found |
| Value Iteration | Single Bellman max update â€” faster convergence |

**What you'll do:** Animated convergence plots, PI vs VI comparison table, Î³ effect on convergence speed, stochastic policy comparison (5 slip values side by side).

### ğŸ“¦ Required Libraries
```
gymnasium    - RL environments
numpy        - numerical computation
matplotlib   - plotting and visualization
seaborn      - heatmaps for value functions
pandas       - data tables
```
All pre-installed on Google Colab. Just click "Open in Colab" and run!

---

## ğŸŒ Interactive Web Apps

### Ch2: MDP Explorer
[â–¶ Launch App](https://mlnjsh.github.io/rl-book-labs/ch2/)

Three modes: **ğŸ” Explore** (click cells â†’ see S,A,R,P), **Ï€ Policy** (click to change arrows), **V Value** (heatmap). Features: deterministic/stochastic toggle, editable grid, robot episodes, Q-value inspector.

### Ch3: Policy Iteration Visualizer
[â–¶ Launch App](https://mlnjsh.github.io/rl-book-labs/ch3/)

Step through PI on FrozenLake: **â‘  One Eval Sweep** (cells light up blue), **â‘¡ Improve** (arrows flash green), **ğŸ¤– Run Robot** (animated walk). Speed control, Î³ and slip sliders.

---

## ğŸ—ï¸ Project Structure

```
rl-book-labs/
â”œâ”€â”€ README.md
â”œâ”€â”€ Ch2_MDP_Environments_Lab.ipynb      # ğŸ““ Colab: 7 MDP environments
â”œâ”€â”€ Ch3_Dynamic_Programming_Lab.ipynb   # ğŸ““ Colab: PI, VI, convergence
â”œâ”€â”€ ch2/
â”‚   â””â”€â”€ index.html                      # ğŸŒ Web: MDP Explorer
â”œâ”€â”€ ch3/
â”‚   â””â”€â”€ index.html                      # ğŸŒ Web: Policy Iteration
â”œâ”€â”€ ch4/                                # (coming soon)
â””â”€â”€ ch5/                                # (coming soon)
```

---

## ğŸ“ About the Book

**Complete Reinforcement Learning Journey: From Basics to RLHF**

The only book that takes you from "What is a Markov Decision Process?" all the way to "How do we align language models with human values?" â€” with intuition, math, code, and interactive labs at every step.

### Key Features
- ğŸ“– **Intuition â†’ Math â†’ Code** triple for every concept
- ğŸ¤– **DeliBot** running example that grows with the theory
- ğŸ§  **Think Like an Agent** boxes for building intuition
- âš ï¸ **Common Misconceptions** boxes to prevent errors
- ğŸ”¬ **Interactive Labs** (this repo!) for hands-on learning
- ğŸ““ **Colab Notebooks** for coding along
- ğŸ“ **Quizzes with detailed answer keys** for each chapter

---

## ğŸ¤ Contributing

Found a bug? Have an idea for a new visualization? Contributions welcome!

1. Fork the repo
2. Create a branch (`git checkout -b feature/new-lab`)
3. Commit your changes
4. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.

---

<p align="center">
  <i>Built with â¤ï¸ as a companion to the book.</i><br>
  <i>"The best way to learn an algorithm is to watch it think."</i>
</p>
