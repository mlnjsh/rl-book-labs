{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Chapter 3: Estimating Value Functions ‚Äî Hands-On Lab\n",
    "\n",
    "**Complete Reinforcement Learning Journey: From Basics to RLHF**\n",
    "\n",
    "In this notebook, you will:\n",
    "1. **Implement** Policy Evaluation (iterative Bellman updates)\n",
    "2. **Implement** Policy Iteration (evaluate ‚Üí improve loop)\n",
    "3. **Implement** Value Iteration (single-step improvement)\n",
    "4. **Visualize** convergence: watch V(s) evolve sweep by sweep\n",
    "5. **Compare** PI vs VI across 7 environments\n",
    "6. **Experiment** with Œ≥, stochasticity, and grid layouts\n",
    "\n",
    "---\n",
    "\n",
    "üìò **Companion to Chapter 3 of the book**  \n",
    "üîó Interactive web app: [Policy Iteration Visualizer](https://mlnjsh.github.io/rl-book-labs/ch3/)  \n",
    "üîó GitHub: [github.com/mlnjsh/rl-book-labs](https://github.com/mlnjsh/rl-book-labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install gymnasium numpy matplotlib seaborn pandas --quiet\n",
    "\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ Libraries used in this notebook:                        ‚îÇ\n",
    "# ‚îÇ   gymnasium    - RL environments (FrozenLake)           ‚îÇ\n",
    "# ‚îÇ   numpy        - numerical computation                  ‚îÇ\n",
    "# ‚îÇ   matplotlib   - plotting and visualization             ‚îÇ\n",
    "# ‚îÇ   seaborn      - heatmaps                               ‚îÇ\n",
    "# ‚îÇ   pandas       - data tables                            ‚îÇ\n",
    "# ‚îÇ   time         - measuring convergence speed            ‚îÇ\n",
    "# ‚îÇ   IPython      - display utilities (built-in)           ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = '#0f172a'\n",
    "plt.rcParams['axes.facecolor'] = '#1e293b'\n",
    "plt.rcParams['text.color'] = '#e2e8f0'\n",
    "plt.rcParams['axes.labelcolor'] = '#e2e8f0'\n",
    "plt.rcParams['xtick.color'] = '#94a3b8'\n",
    "plt.rcParams['ytick.color'] = '#94a3b8'\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "print(\"‚úÖ All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Environment Setup\n",
    "\n",
    "We reuse the MDP framework from Chapter 2 and add all 7 environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MDP BASE ====================\n",
    "class MDP:\n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_transitions(self, s, a):\n",
    "        results = []\n",
    "        for prob, ns in self.transitions.get((s, a), []):\n",
    "            reward = self.rewards.get((s, a, ns), 0)\n",
    "            results.append((prob, ns, reward))\n",
    "        return results\n",
    "\n",
    "# ==================== GRIDWORLD 5x5 ====================\n",
    "class GridWorld(MDP):\n",
    "    DELTAS = {0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)}\n",
    "    ARROWS = {0:'‚Üê', 1:'‚Üì', 2:'‚Üí', 3:'‚Üë'}\n",
    "\n",
    "    def __init__(self, gamma=0.9, slip=0.0):\n",
    "        self.size=5; self.walls={(1,1),(1,3),(3,1)}; self.goal=(4,4); self.pit=(3,4); self.start=(0,0); self.slip=slip\n",
    "        states=[(r,c) for r in range(5) for c in range(5) if (r,c) not in self.walls]\n",
    "        am,tr,rw={},{},{}\n",
    "        for s in states:\n",
    "            if s==self.goal or s==self.pit: am[s]=[]; continue\n",
    "            am[s]=[0,1,2,3]\n",
    "            for a in range(4):\n",
    "                tl=[]\n",
    "                for aa in range(4):\n",
    "                    if slip==0:\n",
    "                        if aa!=a: continue\n",
    "                        p=1.0\n",
    "                    else: p=(1-slip+slip/4) if aa==a else slip/4\n",
    "                    if p<1e-9: continue\n",
    "                    dr,dc=self.DELTAS[aa]; nr,nc=s[0]+dr,s[1]+dc\n",
    "                    if nr<0 or nr>=5 or nc<0 or nc>=5 or (nr,nc) in self.walls: nr,nc=s\n",
    "                    ns=(nr,nc)\n",
    "                    found=False\n",
    "                    for i,(pp,ens) in enumerate(tl):\n",
    "                        if ens==ns: tl[i]=(pp+p,ns); found=True; break\n",
    "                    if not found: tl.append((p,ns))\n",
    "                    rw[(s,a,ns)] = 10.0 if ns==self.goal else (-10.0 if ns==self.pit else -0.1)\n",
    "                tr[(s,a)]=tl\n",
    "        super().__init__(states,am,tr,rw,gamma)\n",
    "\n",
    "# ==================== FROZENLAKE 4x4 ====================\n",
    "class FrozenLakeMDP(MDP):\n",
    "    MAP=[['S','F','F','F'],['F','H','F','H'],['F','F','F','H'],['H','F','F','G']]\n",
    "    DELTAS={0:(0,-1),1:(1,0),2:(0,1),3:(-1,0)}; ARROWS={0:'‚Üê',1:'‚Üì',2:'‚Üí',3:'‚Üë'}\n",
    "    def __init__(self, gamma=0.95, is_slippery=True):\n",
    "        self.size=4; self.is_slippery=is_slippery\n",
    "        self.holes=set(); self.goal=None; self.start=None\n",
    "        for r in range(4):\n",
    "            for c in range(4):\n",
    "                if self.MAP[r][c]=='H': self.holes.add((r,c))\n",
    "                elif self.MAP[r][c]=='G': self.goal=(r,c)\n",
    "                elif self.MAP[r][c]=='S': self.start=(r,c)\n",
    "        states=[(r,c) for r in range(4) for c in range(4)]\n",
    "        terminals=self.holes|{self.goal}; am,tr,rw={},{},{}\n",
    "        for s in states:\n",
    "            if s in terminals: am[s]=[]; continue\n",
    "            am[s]=[0,1,2,3]\n",
    "            for a in range(4):\n",
    "                tl=[]; possible=[(a-1)%4,a,(a+1)%4] if is_slippery else [a]\n",
    "                for aa in possible:\n",
    "                    p=1/3 if is_slippery else 1.0\n",
    "                    dr,dc=self.DELTAS[aa]; nr,nc=s[0]+dr,s[1]+dc\n",
    "                    if nr<0 or nr>=4 or nc<0 or nc>=4: nr,nc=s\n",
    "                    ns=(nr,nc)\n",
    "                    found=False\n",
    "                    for i,(pp,ens) in enumerate(tl):\n",
    "                        if ens==ns: tl[i]=(pp+p,ns); found=True; break\n",
    "                    if not found: tl.append((p,ns))\n",
    "                    rw[(s,a,ns)]=1.0 if ns==self.goal else (-1.0 if ns in self.holes else -0.01)\n",
    "                tr[(s,a)]=tl\n",
    "        super().__init__(states,am,tr,rw,gamma)\n",
    "\n",
    "# ==================== TRAFFIC, THERMOSTAT, BANDIT, INVENTORY, ROBOT ====================\n",
    "class TrafficLightMDP(MDP):\n",
    "    def __init__(self, gamma=0.9):\n",
    "        S=[(t,p) for t in ['low','medium','high'] for p in ['green_NS','green_EW']]\n",
    "        am={s:['keep','switch'] for s in S}; tr={}; rw={}\n",
    "        for t in ['low','medium','high']:\n",
    "            for ph in ['green_NS','green_EW']:\n",
    "                s=(t,ph)\n",
    "                for a in ['keep','switch']:\n",
    "                    np2=ph if a=='keep' else('green_EW' if ph=='green_NS' else 'green_NS')\n",
    "                    if t=='low': tr[(s,a)]=[(0.7,('low',np2)),(0.3,('medium',np2))]; rw[(s,a,('low',np2))]=1.0; rw[(s,a,('medium',np2))]=0.0\n",
    "                    elif t=='medium':\n",
    "                        if a=='switch': tr[(s,a)]=[(0.4,('low',np2)),(0.5,('medium',np2)),(0.1,('high',np2))]\n",
    "                        else: tr[(s,a)]=[(0.2,('low',np2)),(0.4,('medium',np2)),(0.4,('high',np2))]\n",
    "                        rw[(s,a,('low',np2))]=1.0; rw[(s,a,('medium',np2))]=-0.5; rw[(s,a,('high',np2))]=-2.0\n",
    "                    else:\n",
    "                        if a=='switch': tr[(s,a)]=[(0.3,('medium',np2)),(0.5,('high',np2)),(0.2,('low',np2))]\n",
    "                        else: tr[(s,a)]=[(0.1,('medium',np2)),(0.9,('high',np2))]\n",
    "                        rw[(s,a,('medium',np2))]=-0.5; rw[(s,a,('high',np2))]=-3.0; rw[(s,a,('low',np2))]=1.0\n",
    "        super().__init__(S,am,tr,rw,gamma)\n",
    "\n",
    "class ThermostatMDP(MDP):\n",
    "    def __init__(self, gamma=0.9):\n",
    "        S=['cold','comfortable','hot']; am={s:['heat','cool','off'] for s in S}; tr={}; rw={}\n",
    "        tr[('cold','heat')]=[(0.8,'comfortable'),(0.2,'cold')]\n",
    "        tr[('cold','cool')]=[(0.95,'cold'),(0.05,'comfortable')]\n",
    "        tr[('cold','off')]=[(0.7,'cold'),(0.3,'comfortable')]\n",
    "        tr[('comfortable','heat')]=[(0.6,'comfortable'),(0.4,'hot')]\n",
    "        tr[('comfortable','cool')]=[(0.6,'comfortable'),(0.4,'cold')]\n",
    "        tr[('comfortable','off')]=[(0.8,'comfortable'),(0.1,'cold'),(0.1,'hot')]\n",
    "        tr[('hot','heat')]=[(0.2,'comfortable'),(0.8,'hot')]\n",
    "        tr[('hot','cool')]=[(0.8,'comfortable'),(0.2,'hot')]\n",
    "        tr[('hot','off')]=[(0.3,'comfortable'),(0.7,'hot')]\n",
    "        for s in S:\n",
    "            for a in ['heat','cool','off']:\n",
    "                for p,ns in tr[(s,a)]:\n",
    "                    r=2.0 if ns=='comfortable' else -1.0\n",
    "                    if a in ['heat','cool']: r-=0.5\n",
    "                    rw[(s,a,ns)]=r\n",
    "        super().__init__(S,am,tr,rw,gamma)\n",
    "\n",
    "class BanditMDP(MDP):\n",
    "    def __init__(self, gamma=0.9):\n",
    "        S=['morning','afternoon','evening']; am={s:['A','B','C'] for s in S}; tr={}; rw={}\n",
    "        wp={('morning','A'):0.7,('morning','B'):0.3,('morning','C'):0.5,('afternoon','A'):0.4,('afternoon','B'):0.6,('afternoon','C'):0.5,('evening','A'):0.2,('evening','B'):0.5,('evening','C'):0.8}\n",
    "        nx={'morning':'afternoon','afternoon':'evening','evening':'morning'}\n",
    "        for s in S:\n",
    "            for a in ['A','B','C']:\n",
    "                ns=nx[s]; tr[(s,a)]=[(1.0,ns)]; rw[(s,a,ns)]=wp[(s,a)]*10-(1-wp[(s,a)])*2\n",
    "        super().__init__(S,am,tr,rw,gamma)\n",
    "\n",
    "class InventoryMDP(MDP):\n",
    "    def __init__(self, gamma=0.9):\n",
    "        S=list(range(5)); am={s:['o0','o1','o2'] for s in S}; tr={}; rw={}\n",
    "        dp={0:0.3,1:0.5,2:0.2}\n",
    "        for stk in S:\n",
    "            for a in ['o0','o1','o2']:\n",
    "                oq=int(a[1]); ao=min(stk+oq,4); tl=[]\n",
    "                for d,dpr in dp.items():\n",
    "                    sold=min(d,ao); unmet=d-sold; ns2=ao-sold\n",
    "                    r=sold*3-oq*1-ns2*0.5+unmet*(-4)\n",
    "                    found=False\n",
    "                    for i,(pp,ens) in enumerate(tl):\n",
    "                        if ens==ns2: tl[i]=(pp+dpr,ns2); found=True; break\n",
    "                    if not found: tl.append((dpr,ns2))\n",
    "                    rw[(stk,a,ns2)]=r\n",
    "                tr[(stk,a)]=tl\n",
    "        super().__init__(S,am,tr,rw,gamma)\n",
    "\n",
    "class RobotRoomsMDP(MDP):\n",
    "    def __init__(self, gamma=0.9, lock_prob=0.2):\n",
    "        S=['A','B','C','D']; nb={'A':['B','C'],'B':['A','D'],'C':['A','D'],'D':['B','C']}\n",
    "        am={}; tr={}; rw={}\n",
    "        for s in S:\n",
    "            acts=[f'go_{n}' for n in nb[s]]+['stay']; am[s]=acts\n",
    "            for a in acts:\n",
    "                if a=='stay':\n",
    "                    tr[(s,a)]=[(1.0,s)]; rw[(s,a,s)]=1.0 if s=='C' else -0.1\n",
    "                else:\n",
    "                    tgt=a.split('_')[1]\n",
    "                    tr[(s,a)]=[(1-lock_prob,tgt),(lock_prob,s)]\n",
    "                    rw[(s,a,tgt)]=10.0 if tgt=='D' else -0.5; rw[(s,a,s)]=-0.5\n",
    "        super().__init__(S,am,tr,rw,gamma)\n",
    "\n",
    "print(\"‚úÖ All 7 environments loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìê Algorithm 1: Policy Evaluation (Iterative)\n",
    "\n",
    "Given a policy œÄ, compute V^œÄ(s) by repeatedly applying the Bellman expectation equation:\n",
    "\n",
    "$$V_{k+1}(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s,\\pi(s),s') + \\gamma V_k(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, theta=1e-8, max_iter=1000, track_history=False):\n",
    "    \"\"\"Iterative policy evaluation.\n",
    "    Returns: V dict, and optionally history of V at each sweep.\"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    history = [V.copy()] if track_history else None\n",
    "    deltas = []\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            if s not in policy or not mdp.actions.get(s):\n",
    "                continue\n",
    "            a = policy[s]\n",
    "            new_v = sum(p * (r + mdp.gamma * V[ns]) for p, ns, r in mdp.get_transitions(s, a))\n",
    "            delta = max(delta, abs(V[s] - new_v))\n",
    "            V[s] = new_v\n",
    "\n",
    "        deltas.append(delta)\n",
    "        if track_history:\n",
    "            history.append(V.copy())\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, deltas, history\n",
    "\n",
    "# Demo: evaluate a random policy on GridWorld\n",
    "gw = GridWorld(gamma=0.9, slip=0.0)\n",
    "random_policy = {s: np.random.choice(gw.actions[s]) for s in gw.states if gw.actions.get(s)}\n",
    "\n",
    "V, deltas, hist = policy_evaluation(gw, random_policy, track_history=True)\n",
    "\n",
    "print(f\"Converged in {len(deltas)} sweeps\")\n",
    "print(f\"Final max delta: {deltas[-1]:.2e}\")\n",
    "\n",
    "# Plot convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.semilogy(deltas, color='#22d3ee', linewidth=2)\n",
    "ax.axhline(y=1e-8, color='#ef4444', linestyle='--', alpha=0.5, label='Œ∏ threshold')\n",
    "ax.set_xlabel('Sweep'); ax.set_ylabel('Max |ŒîV|')\n",
    "ax.set_title('Policy Evaluation Convergence', fontweight='bold', color='#22d3ee')\n",
    "ax.legend(); ax.grid(alpha=0.1)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Animated Sweep-by-Sweep Convergence\n",
    "\n",
    "Watch the value function evolve as policy evaluation runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gridworld_values(gw, V, policy=None, title=\"\"):\n",
    "    \"\"\"Quick grid visualization.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.set_xlim(-0.5,4.5); ax.set_ylim(-0.5,4.5); ax.set_aspect('equal'); ax.invert_yaxis()\n",
    "    all_v = [V.get(s,0) for s in gw.states if s not in gw.walls and s!=gw.goal and s!=gw.pit]\n",
    "    mn,mx = (min(all_v),max(all_v)) if all_v else (0,1)\n",
    "    arrow_map={0:(0,-0.3),1:(0.3,0),2:(0,0.3),3:(-0.3,0)}\n",
    "    for r in range(5):\n",
    "        for c in range(5):\n",
    "            if (r,c) in gw.walls:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#334155')); ax.text(c,r,'üß±',ha='center',va='center',fontsize=16)\n",
    "            elif (r,c)==gw.goal:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#064e3b',alpha=0.5)); ax.text(c,r,'üèÜ',ha='center',va='center',fontsize=16)\n",
    "            elif (r,c)==gw.pit:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#450a0a',alpha=0.5)); ax.text(c,r,'üï≥Ô∏è',ha='center',va='center',fontsize=16)\n",
    "            else:\n",
    "                v=V.get((r,c),0); t=(v-mn)/(mx-mn+1e-8)\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color=plt.cm.RdYlGn(t),alpha=0.4))\n",
    "                ax.text(c,r+0.3,f\"{v:.2f}\",ha='center',va='center',fontsize=8,color='white',fontweight='bold')\n",
    "                if policy and (r,c) in policy:\n",
    "                    dy,dx=arrow_map[policy[(r,c)]]\n",
    "                    ax.annotate('',xy=(c+dx,r+dy),xytext=(c,r),arrowprops=dict(arrowstyle='->',color='#22d3ee',lw=2))\n",
    "            ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,fill=False,edgecolor='#334155',lw=0.3))\n",
    "    ax.set_title(title,fontsize=12,fontweight='bold',color='#22d3ee',pad=8)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for s in ax.spines.values(): s.set_visible(False)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Show V at sweeps 0, 1, 5, 20, final\n",
    "sweep_indices = [0, 1, 5, 20, len(hist)-1]\n",
    "for idx in sweep_indices:\n",
    "    if idx < len(hist):\n",
    "        show_gridworld_values(gw, hist[idx], title=f\"V after sweep {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìê Algorithm 2: Policy Iteration\n",
    "\n",
    "1. **Evaluate** current policy ‚Üí V^œÄ\n",
    "2. **Improve** policy ‚Üí greedy w.r.t. V^œÄ\n",
    "3. Repeat until stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, verbose=True):\n",
    "    \"\"\"Full Policy Iteration. Returns V*, œÄ*, and tracking info.\"\"\"\n",
    "    # Initialize random policy\n",
    "    policy = {s: mdp.actions[s][0] for s in mdp.states if mdp.actions.get(s)}\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    track = []\n",
    "\n",
    "    for iteration in range(100):\n",
    "        # Evaluate\n",
    "        V, deltas, _ = policy_evaluation(mdp, policy)\n",
    "        eval_sweeps = len(deltas)\n",
    "\n",
    "        # Improve\n",
    "        changed = 0\n",
    "        new_policy = {}\n",
    "        for s in mdp.states:\n",
    "            if not mdp.actions.get(s): continue\n",
    "            best_a, best_v = None, -float('inf')\n",
    "            for a in mdp.actions[s]:\n",
    "                q = sum(p*(r+mdp.gamma*V[ns]) for p,ns,r in mdp.get_transitions(s,a))\n",
    "                if q > best_v: best_v=q; best_a=a\n",
    "            if best_a != policy.get(s): changed += 1\n",
    "            new_policy[s] = best_a\n",
    "\n",
    "        track.append({'iter': iteration+1, 'eval_sweeps': eval_sweeps, 'changes': changed})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Iter {iteration+1}: {eval_sweeps} eval sweeps, {changed} policy changes\")\n",
    "\n",
    "        if changed == 0:\n",
    "            if verbose: print(f\"  ‚úÖ Converged in {iteration+1} iterations!\")\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return V, policy, track\n",
    "\n",
    "# Run on GridWorld\n",
    "print(\"üåç Policy Iteration on GridWorld 5√ó5\")\n",
    "print(\"=\"*50)\n",
    "gw = GridWorld(gamma=0.9, slip=0.0)\n",
    "V_pi, pi_pi, track_pi = policy_iteration(gw)\n",
    "show_gridworld_values(gw, V_pi, pi_pi, \"GridWorld ‚Äî Optimal Policy (PI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìê Algorithm 3: Value Iteration\n",
    "\n",
    "Combines evaluation and improvement in a single step:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, theta=1e-8, max_iter=1000, verbose=True):\n",
    "    \"\"\"Value Iteration. Returns V*, œÄ*.\"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "    deltas = []\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            if not mdp.actions.get(s): continue\n",
    "            old_v = V[s]\n",
    "            V[s] = max(\n",
    "                sum(p*(r+mdp.gamma*V[ns]) for p,ns,r in mdp.get_transitions(s,a))\n",
    "                for a in mdp.actions[s]\n",
    "            )\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "        deltas.append(delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract policy\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        if not mdp.actions.get(s): continue\n",
    "        best_a, best_v = None, -float('inf')\n",
    "        for a in mdp.actions[s]:\n",
    "            q = sum(p*(r+mdp.gamma*V[ns]) for p,ns,r in mdp.get_transitions(s,a))\n",
    "            if q > best_v: best_v=q; best_a=a\n",
    "        policy[s] = best_a\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  ‚úÖ Value Iteration converged in {len(deltas)} sweeps\")\n",
    "    return V, policy, deltas\n",
    "\n",
    "# Run on GridWorld\n",
    "print(\"üåç Value Iteration on GridWorld 5√ó5\")\n",
    "print(\"=\"*50)\n",
    "gw = GridWorld(gamma=0.9, slip=0.0)\n",
    "V_vi, pi_vi, deltas_vi = value_iteration(gw)\n",
    "show_gridworld_values(gw, V_vi, pi_vi, \"GridWorld ‚Äî Optimal Policy (VI)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Policy Iteration vs Value Iteration ‚Äî All Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "    'GridWorld (det)': GridWorld(0.9, 0.0),\n",
    "    'GridWorld (sto)': GridWorld(0.9, 0.2),\n",
    "    'FrozenLake (slip)': FrozenLakeMDP(0.95, True),\n",
    "    'FrozenLake (det)': FrozenLakeMDP(0.95, False),\n",
    "    'Traffic Light': TrafficLightMDP(0.9),\n",
    "    'Thermostat': ThermostatMDP(0.9),\n",
    "    'Bandit': BanditMDP(0.9),\n",
    "    'Inventory': InventoryMDP(0.9),\n",
    "    'Robot Rooms': RobotRoomsMDP(0.9, 0.2),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, mdp in envs.items():\n",
    "    # PI\n",
    "    t0 = time.time()\n",
    "    V_pi, pi_pi, track = policy_iteration(mdp, verbose=False)\n",
    "    pi_time = time.time() - t0\n",
    "    pi_iters = len(track)\n",
    "\n",
    "    # VI\n",
    "    t0 = time.time()\n",
    "    V_vi, pi_vi, deltas = value_iteration(mdp, verbose=False)\n",
    "    vi_time = time.time() - t0\n",
    "    vi_sweeps = len(deltas)\n",
    "\n",
    "    # Check same policy\n",
    "    same = all(pi_pi.get(s) == pi_vi.get(s) for s in pi_pi)\n",
    "\n",
    "    results.append({\n",
    "        'Environment': name,\n",
    "        '|S|': len(mdp.states),\n",
    "        'PI Iters': pi_iters,\n",
    "        'PI Time (ms)': f\"{pi_time*1000:.1f}\",\n",
    "        'VI Sweeps': vi_sweeps,\n",
    "        'VI Time (ms)': f\"{vi_time*1000:.1f}\",\n",
    "        'Same œÄ*?': '‚úÖ' if same else '‚ùå'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Policy Iteration vs Value Iteration ‚Äî Comparison\")\n",
    "print(\"=\"*80)\n",
    "display(df)\n",
    "print(\"\\nüí° Both always find the same optimal policy!\")\n",
    "print(\"   PI uses fewer iterations but each iteration has a full evaluation phase.\")\n",
    "print(\"   VI uses more sweeps but each sweep is simpler (just one Bellman max).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Experiment: How Œ≥ Affects Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.1, 0.5, 0.9, 0.95, 0.99]\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for g in gammas:\n",
    "    gw = GridWorld(gamma=g, slip=0.0)\n",
    "    _, _, deltas = value_iteration(gw, verbose=False)\n",
    "    ax.semilogy(deltas, label=f'Œ≥={g}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Sweep'); ax.set_ylabel('Max |ŒîV|')\n",
    "ax.set_title('Value Iteration Convergence vs Œ≥', fontweight='bold', color='#22d3ee')\n",
    "ax.legend(); ax.grid(alpha=0.1)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"üí° Higher Œ≥ ‚Üí slower convergence! The agent must propagate values across more steps.\")\n",
    "print(\"   Œ≥=0.99 takes ~4x more sweeps than Œ≥=0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Experiment: Deterministic vs Stochastic Optimal Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slips = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "for s in slips:\n",
    "    gw = GridWorld(gamma=0.9, slip=s)\n",
    "    V, pi, _ = value_iteration(gw, verbose=False)\n",
    "    show_gridworld_values(gw, V, pi, f\"Optimal Policy ‚Äî slip={s:.1f}\")\n",
    "\n",
    "print(\"üí° As slip increases:\")\n",
    "print(\"   - Policy becomes more cautious near pits\")\n",
    "print(\"   - Values decrease (less certain about reaching goal)\")\n",
    "print(\"   - Some cells change arrow direction to avoid risky paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Q-Value Analysis for All Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_values(mdp, V):\n",
    "    Q = {}\n",
    "    for s in mdp.states:\n",
    "        for a in mdp.actions.get(s, []):\n",
    "            Q[(s,a)] = sum(p*(r+mdp.gamma*V[ns]) for p,ns,r in mdp.get_transitions(s,a))\n",
    "    return Q\n",
    "\n",
    "# Q-values for Thermostat\n",
    "print(\"üå°Ô∏è Thermostat Q-Values\")\n",
    "thermo = ThermostatMDP(0.9)\n",
    "V_th, pi_th, _ = value_iteration(thermo, verbose=False)\n",
    "Q_th = compute_q_values(thermo, V_th)\n",
    "\n",
    "rows = []\n",
    "for s in ['cold','comfortable','hot']:\n",
    "    row = {'State': s}\n",
    "    for a in ['heat','cool','off']:\n",
    "        q = Q_th.get((s,a), 0)\n",
    "        best = q == max(Q_th.get((s,a2),0) for a2 in ['heat','cool','off'])\n",
    "        row[a] = f\"{q:.2f}\" + (\" ‚òÖ\" if best else \"\")\n",
    "    rows.append(row)\n",
    "display(pd.DataFrame(rows).set_index('State'))\n",
    "\n",
    "print(f\"\\nOptimal: cold‚Üí{pi_th['cold']}, comfortable‚Üí{pi_th['comfortable']}, hot‚Üí{pi_th['hot']}\")\n",
    "\n",
    "# Q-values for GridWorld start state\n",
    "print(\"\\nüåç GridWorld Q-Values at START (0,0)\")\n",
    "gw = GridWorld(0.9, 0.0)\n",
    "V_gw, pi_gw, _ = value_iteration(gw, verbose=False)\n",
    "Q_gw = compute_q_values(gw, V_gw)\n",
    "\n",
    "for a in range(4):\n",
    "    q = Q_gw.get(((0,0),a), 0)\n",
    "    best = q == max(Q_gw.get(((0,0),i),0) for i in range(4))\n",
    "    print(f\"  {GridWorld.ARROWS[a]} Q((0,0),{a}) = {q:.4f}{'  ‚òÖ BEST' if best else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Summary\n",
    "\n",
    "### Algorithms Implemented\n",
    "| Algorithm | What It Does | Key Equation |\n",
    "|-----------|-------------|-------------|\n",
    "| Policy Evaluation | Computes V^œÄ for a given œÄ | V(s) ‚Üê Œ£ P[R + Œ≥V(s')] |\n",
    "| Policy Iteration | Finds œÄ* via evaluate‚Üíimprove loop | Guaranteed to converge |\n",
    "| Value Iteration | Finds V* via Bellman optimality | V(s) ‚Üê max_a Œ£ P[R + Œ≥V(s')] |\n",
    "\n",
    "### Key Findings\n",
    "1. **Both PI and VI find the same optimal policy** for all 7 environments\n",
    "2. **Higher Œ≥ ‚Üí slower convergence** but better long-term planning\n",
    "3. **Stochastic environments ‚Üí cautious policies** that avoid risky states\n",
    "4. **Q-values** reveal exactly why each action is good or bad\n",
    "5. **These are DP methods** ‚Äî they need the full model P(s'|s,a)\n",
    "\n",
    "### What's Next?\n",
    "- **Chapter 4**: Monte Carlo methods ‚Äî learn V without knowing P!\n",
    "- **Chapter 5**: TD Learning ‚Äî learn from incomplete episodes\n",
    "\n",
    "---\n",
    "üìò **Book**: Complete Reinforcement Learning Journey  \n",
    "üîó [Interactive Labs](https://mlnjsh.github.io/rl-book-labs/)  \n",
    "üîó [GitHub](https://github.com/mlnjsh/rl-book-labs)"
   ]
  }
 ]
}
