{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ Chapter 2: Markov Decision Processes â€” Hands-On Lab\n",
    "\n",
    "**Complete Reinforcement Learning Journey: From Basics to RLHF**\n",
    "\n",
    "In this notebook, you will:\n",
    "1. **Build 7 MDP environments** from scratch\n",
    "2. **Visualize** states, actions, transitions, and rewards\n",
    "3. **Understand** deterministic vs stochastic dynamics\n",
    "4. **Compute** state-value V(s) and action-value Q(s,a)\n",
    "5. **See** how Î³ (discount factor) changes optimal behavior\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“˜ **Companion to Chapter 2 of the book**  \n",
    "ğŸ”— Interactive web app: [MDP Explorer](https://mlnjsh.github.io/rl-book-labs/ch2/)  \n",
    "ğŸ”— GitHub: [github.com/mlnjsh/rl-book-labs](https://github.com/mlnjsh/rl-book-labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run once)\n",
    "!pip install gymnasium numpy matplotlib seaborn pandas --quiet\n",
    "\n",
    "# All libraries used in this notebook:\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "# â”‚ gymnasium    - RL environments (FrozenLake, etc.)       â”‚\n",
    "# â”‚ numpy        - numerical computation                    â”‚\n",
    "# â”‚ matplotlib   - plotting and visualization               â”‚\n",
    "# â”‚ seaborn      - heatmaps for value functions             â”‚\n",
    "# â”‚ pandas       - tables for transition probabilities      â”‚\n",
    "# â”‚ IPython      - display utilities (built-in)             â”‚\n",
    "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style\n",
    "plt.rcParams['figure.facecolor'] = '#0f172a'\n",
    "plt.rcParams['axes.facecolor'] = '#1e293b'\n",
    "plt.rcParams['text.color'] = '#e2e8f0'\n",
    "plt.rcParams['axes.labelcolor'] = '#e2e8f0'\n",
    "plt.rcParams['xtick.color'] = '#94a3b8'\n",
    "plt.rcParams['ytick.color'] = '#94a3b8'\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "print(\"âœ… All libraries loaded!\")\n",
    "print(f\"   gymnasium: {gym.__version__}\")\n",
    "print(f\"   numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ—ï¸ MDP Framework\n",
    "\n",
    "Every MDP is a tuple **M = âŸ¨S, A, P, R, Î³âŸ©**:\n",
    "- **S** â€” set of states\n",
    "- **A** â€” set of actions\n",
    "- **P(s'|s,a)** â€” transition probabilities\n",
    "- **R(s,a,s')** â€” reward function\n",
    "- **Î³** â€” discount factor\n",
    "\n",
    "Let's build a base class that all our environments will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"Base MDP class that all environments inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        \"\"\"\n",
    "        states: list of state names\n",
    "        actions: dict {state: [available_actions]}\n",
    "        transitions: dict {(state, action): [(prob, next_state), ...]}\n",
    "        rewards: dict {(state, action, next_state): reward}\n",
    "        gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        \"\"\"Returns list of (probability, next_state, reward) tuples.\"\"\"\n",
    "        results = []\n",
    "        for prob, next_state in self.transitions.get((state, action), []):\n",
    "            reward = self.rewards.get((state, action, next_state), 0)\n",
    "            results.append((prob, next_state, reward))\n",
    "        return results\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print the MDP components.\"\"\"\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  MDP Summary\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  States  (|S|={len(self.states)}): {self.states}\")\n",
    "        print(f\"  Actions: {set(a for acts in self.actions.values() for a in acts)}\")\n",
    "        print(f\"  Gamma:   {self.gamma}\")\n",
    "        print(f\"  Transitions: {len(self.transitions)} (state,action) pairs\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    def show_transitions_table(self, state, action):\n",
    "        \"\"\"Show transition table for a (state, action) pair.\"\"\"\n",
    "        ts = self.get_transitions(state, action)\n",
    "        if not ts:\n",
    "            print(f\"  No transitions for ({state}, {action})\")\n",
    "            return\n",
    "        rows = []\n",
    "        for p, ns, r in ts:\n",
    "            rows.append({\"P(s'|s,a)\": f\"{p:.2f}\", \"Next State (s')\": ns, \"Reward R\": r})\n",
    "        df = pd.DataFrame(rows)\n",
    "        print(f\"\\n  State: {state} | Action: {action}\")\n",
    "        print(f\"  {'â”€'*40}\")\n",
    "        display(df)\n",
    "\n",
    "print(\"âœ… MDP base class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸŒ Environment 1: 5Ã—5 GridWorld\n",
    "\n",
    "A classic grid where an agent moves in 4 directions. Walls block movement, there's a goal (+10) and a pit (-10).\n",
    "\n",
    "```\n",
    "ğŸ¤–  Â·   Â·   Â·   Â·\n",
    " Â·  ğŸ§±   Â·  ğŸ§±   Â·\n",
    " Â·   Â·   Â·   Â·   Â·\n",
    " Â·  ğŸ§±   Â·   Â·  ğŸ•³ï¸\n",
    " Â·   Â·   Â·   Â·  ğŸ†\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(MDP):\n",
    "    \"\"\"5x5 GridWorld with walls, goal, and pit.\"\"\"\n",
    "\n",
    "    ACTION_NAMES = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}\n",
    "    DELTAS = {0: (0,-1), 1: (1,0), 2: (0,1), 3: (-1,0)}\n",
    "\n",
    "    def __init__(self, gamma=0.9, slip=0.0):\n",
    "        self.size = 5\n",
    "        self.walls = {(1,1), (1,3), (3,1)}\n",
    "        self.goal = (4,4)\n",
    "        self.pit = (3,4)\n",
    "        self.start = (0,0)\n",
    "        self.slip = slip\n",
    "\n",
    "        # Build MDP components\n",
    "        states = [(r,c) for r in range(5) for c in range(5) if (r,c) not in self.walls]\n",
    "        actions_map = {}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        for s in states:\n",
    "            if s == self.goal or s == self.pit:\n",
    "                actions_map[s] = []\n",
    "                continue\n",
    "            actions_map[s] = [0, 1, 2, 3]\n",
    "            for a in range(4):\n",
    "                trans_list = []\n",
    "                for actual_a in range(4):\n",
    "                    if slip == 0:\n",
    "                        if actual_a != a:\n",
    "                            continue\n",
    "                        prob = 1.0\n",
    "                    else:\n",
    "                        prob = (1 - slip + slip/4) if actual_a == a else slip/4\n",
    "                    if prob < 1e-9:\n",
    "                        continue\n",
    "\n",
    "                    dr, dc = self.DELTAS[actual_a]\n",
    "                    nr, nc = s[0]+dr, s[1]+dc\n",
    "                    if nr<0 or nr>=5 or nc<0 or nc>=5 or (nr,nc) in self.walls:\n",
    "                        nr, nc = s\n",
    "                    ns = (nr, nc)\n",
    "\n",
    "                    # Merge duplicate next states\n",
    "                    found = False\n",
    "                    for i, (p, existing_ns) in enumerate(trans_list):\n",
    "                        if existing_ns == ns:\n",
    "                            trans_list[i] = (p + prob, ns)\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        trans_list.append((prob, ns))\n",
    "\n",
    "                    # Rewards\n",
    "                    if ns == self.goal:\n",
    "                        rewards[(s, a, ns)] = 10.0\n",
    "                    elif ns == self.pit:\n",
    "                        rewards[(s, a, ns)] = -10.0\n",
    "                    else:\n",
    "                        rewards[(s, a, ns)] = -0.1\n",
    "\n",
    "                transitions[(s, a)] = trans_list\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "    def visualize(self, values=None, policy=None, title=\"GridWorld 5Ã—5\"):\n",
    "        \"\"\"Visualize the grid with optional value heatmap and policy arrows.\"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "        ax.set_xlim(-0.5, 4.5)\n",
    "        ax.set_ylim(-0.5, 4.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        arrow_map = {0: (0, -0.3), 1: (0.3, 0), 2: (0, 0.3), 3: (-0.3, 0)}\n",
    "\n",
    "        for r in range(5):\n",
    "            for c in range(5):\n",
    "                if (r,c) in self.walls:\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='#334155'))\n",
    "                    ax.text(c, r, 'ğŸ§±', ha='center', va='center', fontsize=20)\n",
    "                elif (r,c) == self.goal:\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='#064e3b', alpha=0.5))\n",
    "                    ax.text(c, r, 'ğŸ†', ha='center', va='center', fontsize=20)\n",
    "                elif (r,c) == self.pit:\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='#450a0a', alpha=0.5))\n",
    "                    ax.text(c, r, 'ğŸ•³ï¸', ha='center', va='center', fontsize=20)\n",
    "                else:\n",
    "                    if values is not None and (r,c) in values:\n",
    "                        v = values[(r,c)]\n",
    "                        all_v = [values[s] for s in values]\n",
    "                        mn, mx = min(all_v), max(all_v)\n",
    "                        t = (v - mn) / (mx - mn + 1e-8)\n",
    "                        color = plt.cm.RdYlGn(t)\n",
    "                        ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color=color, alpha=0.4))\n",
    "                        ax.text(c, r+0.3, f\"{v:.2f}\", ha='center', va='center',\n",
    "                                fontsize=8, color='white', fontweight='bold')\n",
    "                    else:\n",
    "                        ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='#1e293b', alpha=0.3))\n",
    "\n",
    "                    if (r,c) == self.start:\n",
    "                        ax.text(c, r-0.15, 'ğŸ¤–', ha='center', va='center', fontsize=16)\n",
    "\n",
    "                    if policy is not None and (r,c) in policy:\n",
    "                        a = policy[(r,c)]\n",
    "                        dy, dx = arrow_map[a]\n",
    "                        ax.annotate('', xy=(c+dx, r+dy), xytext=(c, r),\n",
    "                                   arrowprops=dict(arrowstyle='->', color='#22d3ee', lw=2.5))\n",
    "\n",
    "                # Grid lines\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=False,\n",
    "                            edgecolor='#334155', linewidth=0.5))\n",
    "                ax.text(c+0.35, r-0.35, f\"{r},{c}\", ha='center', va='center',\n",
    "                       fontsize=6, color='#475569')\n",
    "\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold', color='#22d3ee', pad=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Create and visualize\n",
    "gw = GridWorld(gamma=0.9, slip=0.0)\n",
    "gw.print_summary()\n",
    "gw.visualize(title=\"GridWorld 5Ã—5 â€” Deterministic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect transitions for a specific state and action\n",
    "print(\"â•\" * 50)\n",
    "print(\"  DETERMINISTIC (slip=0)\")\n",
    "print(\"â•\" * 50)\n",
    "gw_det = GridWorld(slip=0.0)\n",
    "gw_det.show_transitions_table((0,0), 2)  # State (0,0), Action â†’ (Right)\n",
    "gw_det.show_transitions_table((2,3), 1)  # State (2,3), Action â†“ (Down) â€” near pit!\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 50)\n",
    "print(\"  STOCHASTIC (slip=0.2)\")\n",
    "print(\"â•\" * 50)\n",
    "gw_sto = GridWorld(slip=0.2)\n",
    "gw_sto.show_transitions_table((0,0), 2)  # Same state, same action â€” but now stochastic!\n",
    "gw_sto.show_transitions_table((2,3), 1)  # Near pit â€” see the danger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”‘ Key Insight\n",
    "\n",
    "Notice the difference:\n",
    "- **Deterministic**: Action â†’ always goes where intended (P=1.0)\n",
    "- **Stochastic**: Action â†’ 80% intended, 5% each other direction. Near the pit, this means there's a chance of falling in even with the \"right\" action!\n",
    "\n",
    "This is why **stochastic MDPs require more cautious policies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§Š Environment 2: FrozenLake 4Ã—4\n",
    "\n",
    "The classic Gymnasium environment. Walk across a frozen lake â€” but some tiles are holes!\n",
    "\n",
    "```\n",
    "S  F  F  F\n",
    "F  H  F  H\n",
    "F  F  F  H\n",
    "H  F  F  G\n",
    "```\n",
    "S=Start, F=Frozen, H=Hole, G=Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeMDP(MDP):\n",
    "    \"\"\"FrozenLake 4x4 as an explicit MDP.\"\"\"\n",
    "\n",
    "    MAP = [\n",
    "        ['S','F','F','F'],\n",
    "        ['F','H','F','H'],\n",
    "        ['F','F','F','H'],\n",
    "        ['H','F','F','G']\n",
    "    ]\n",
    "    ACTION_NAMES = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}\n",
    "    DELTAS = {0: (0,-1), 1: (1,0), 2: (0,1), 3: (-1,0)}\n",
    "\n",
    "    def __init__(self, gamma=0.95, is_slippery=True):\n",
    "        self.size = 4\n",
    "        self.is_slippery = is_slippery\n",
    "        self.holes = set()\n",
    "        self.goal = None\n",
    "        self.start = None\n",
    "\n",
    "        for r in range(4):\n",
    "            for c in range(4):\n",
    "                if self.MAP[r][c] == 'H': self.holes.add((r,c))\n",
    "                elif self.MAP[r][c] == 'G': self.goal = (r,c)\n",
    "                elif self.MAP[r][c] == 'S': self.start = (r,c)\n",
    "\n",
    "        states = [(r,c) for r in range(4) for c in range(4)]\n",
    "        terminals = self.holes | {self.goal}\n",
    "        actions_map = {}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        for s in states:\n",
    "            if s in terminals:\n",
    "                actions_map[s] = []\n",
    "                continue\n",
    "            actions_map[s] = [0,1,2,3]\n",
    "            for a in range(4):\n",
    "                trans_list = []\n",
    "                if is_slippery:\n",
    "                    possible = [(a-1)%4, a, (a+1)%4]\n",
    "                else:\n",
    "                    possible = [a]\n",
    "\n",
    "                for actual_a in possible:\n",
    "                    prob = 1/3 if is_slippery else 1.0\n",
    "                    dr, dc = self.DELTAS[actual_a]\n",
    "                    nr, nc = s[0]+dr, s[1]+dc\n",
    "                    if nr<0 or nr>=4 or nc<0 or nc>=4:\n",
    "                        nr, nc = s\n",
    "                    ns = (nr, nc)\n",
    "\n",
    "                    found = False\n",
    "                    for i, (p, existing_ns) in enumerate(trans_list):\n",
    "                        if existing_ns == ns:\n",
    "                            trans_list[i] = (p + prob, ns)\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        trans_list.append((prob, ns))\n",
    "\n",
    "                    if ns == self.goal:\n",
    "                        rewards[(s, a, ns)] = 1.0\n",
    "                    elif ns in self.holes:\n",
    "                        rewards[(s, a, ns)] = -1.0\n",
    "                    else:\n",
    "                        rewards[(s, a, ns)] = -0.01\n",
    "\n",
    "                transitions[(s, a)] = trans_list\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "    def visualize(self, values=None, policy=None, title=\"FrozenLake 4Ã—4\"):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        ax.set_xlim(-0.5, 3.5); ax.set_ylim(-0.5, 3.5)\n",
    "        ax.set_aspect('equal'); ax.invert_yaxis()\n",
    "        arrow_map = {0: (0,-0.3), 1: (0.3,0), 2: (0,0.3), 3: (-0.3,0)}\n",
    "\n",
    "        for r in range(4):\n",
    "            for c in range(4):\n",
    "                cell = self.MAP[r][c]\n",
    "                if cell == 'H':\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#450a0a',alpha=0.5))\n",
    "                    ax.text(c, r, 'ğŸ•³ï¸', ha='center', va='center', fontsize=18)\n",
    "                elif cell == 'G':\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#064e3b',alpha=0.5))\n",
    "                    ax.text(c, r, 'ğŸ†', ha='center', va='center', fontsize=18)\n",
    "                else:\n",
    "                    if values and (r,c) in values:\n",
    "                        v = values[(r,c)]\n",
    "                        all_v = [values[s] for s in values if s not in self.holes]\n",
    "                        mn, mx = min(all_v), max(all_v)\n",
    "                        t = (v-mn)/(mx-mn+1e-8)\n",
    "                        ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color=plt.cm.RdYlGn(t),alpha=0.4))\n",
    "                        ax.text(c, r+0.3, f\"{v:.3f}\", ha='center', va='center', fontsize=7, color='white')\n",
    "                    else:\n",
    "                        ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#1e3a5f',alpha=0.3))\n",
    "                    if cell == 'S':\n",
    "                        ax.text(c, r-0.1, 'ğŸ¤–', ha='center', va='center', fontsize=14)\n",
    "                    if policy and (r,c) in policy:\n",
    "                        dy, dx = arrow_map[policy[(r,c)]]\n",
    "                        ax.annotate('', xy=(c+dx,r+dy), xytext=(c,r),\n",
    "                                   arrowprops=dict(arrowstyle='->',color='#22d3ee',lw=2))\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,fill=False,edgecolor='#334155',lw=0.5))\n",
    "\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold', color='#22d3ee', pad=8)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        for s in ax.spines.values(): s.set_visible(False)\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "fl = FrozenLakeMDP(gamma=0.95, is_slippery=True)\n",
    "fl.print_summary()\n",
    "fl.visualize(title=\"FrozenLake 4Ã—4 â€” Slippery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš¦ Environment 3: Traffic Light Controller\n",
    "\n",
    "An agent controls a traffic light at an intersection. States represent traffic density and light phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightMDP(MDP):\n",
    "    \"\"\"Traffic light controller MDP.\n",
    "    States: (traffic_level, light_phase)\n",
    "      traffic_level: low, medium, high\n",
    "      light_phase: green_NS, green_EW\n",
    "    Actions: keep, switch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9):\n",
    "        traffic_levels = ['low', 'medium', 'high']\n",
    "        phases = ['green_NS', 'green_EW']\n",
    "        states = [(t, p) for t in traffic_levels for p in phases]\n",
    "\n",
    "        actions_map = {s: ['keep', 'switch'] for s in states}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        # Traffic dynamics\n",
    "        for traffic in traffic_levels:\n",
    "            for phase in phases:\n",
    "                s = (traffic, phase)\n",
    "                for action in ['keep', 'switch']:\n",
    "                    new_phase = phase if action == 'keep' else \\\n",
    "                               ('green_EW' if phase == 'green_NS' else 'green_NS')\n",
    "                    trans = []\n",
    "\n",
    "                    if traffic == 'low':\n",
    "                        trans = [(0.7, ('low', new_phase)),\n",
    "                                 (0.3, ('medium', new_phase))]\n",
    "                        rewards[(s, action, ('low', new_phase))] = 1.0\n",
    "                        rewards[(s, action, ('medium', new_phase))] = 0.0\n",
    "                    elif traffic == 'medium':\n",
    "                        if action == 'switch':\n",
    "                            trans = [(0.4, ('low', new_phase)),\n",
    "                                     (0.5, ('medium', new_phase)),\n",
    "                                     (0.1, ('high', new_phase))]\n",
    "                        else:\n",
    "                            trans = [(0.2, ('low', new_phase)),\n",
    "                                     (0.4, ('medium', new_phase)),\n",
    "                                     (0.4, ('high', new_phase))]\n",
    "                        rewards[(s, action, ('low', new_phase))] = 1.0\n",
    "                        rewards[(s, action, ('medium', new_phase))] = -0.5\n",
    "                        rewards[(s, action, ('high', new_phase))] = -2.0\n",
    "                    else:  # high\n",
    "                        if action == 'switch':\n",
    "                            trans = [(0.3, ('medium', new_phase)),\n",
    "                                     (0.5, ('high', new_phase)),\n",
    "                                     (0.2, ('low', new_phase))]\n",
    "                        else:\n",
    "                            trans = [(0.1, ('medium', new_phase)),\n",
    "                                     (0.9, ('high', new_phase))]\n",
    "                        rewards[(s, action, ('medium', new_phase))] = -0.5\n",
    "                        rewards[(s, action, ('high', new_phase))] = -3.0\n",
    "                        rewards[(s, action, ('low', new_phase))] = 1.0\n",
    "\n",
    "                    transitions[(s, action)] = trans\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "traffic = TrafficLightMDP(gamma=0.9)\n",
    "traffic.print_summary()\n",
    "\n",
    "# Show key transitions\n",
    "traffic.show_transitions_table(('high', 'green_NS'), 'keep')\n",
    "traffic.show_transitions_table(('high', 'green_NS'), 'switch')\n",
    "print(\"\\nğŸ’¡ Notice: 'switch' when traffic is high â†’ better chance of reducing congestion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸŒ¡ï¸ Environment 4: Thermostat Controller\n",
    "\n",
    "An agent controls a room thermostat. States are temperatures, actions are heat/cool/off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermostatMDP(MDP):\n",
    "    \"\"\"Thermostat controller.\n",
    "    States: cold (< 18Â°C), comfortable (18-24Â°C), hot (> 24Â°C)\n",
    "    Actions: heat, cool, off\n",
    "    Goal: stay in comfortable zone with minimum energy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9):\n",
    "        states = ['cold', 'comfortable', 'hot']\n",
    "        actions_map = {s: ['heat', 'cool', 'off'] for s in states}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        # Cold state\n",
    "        transitions[('cold','heat')] = [(0.8,'comfortable'), (0.2,'cold')]\n",
    "        transitions[('cold','cool')] = [(0.95,'cold'), (0.05,'comfortable')]\n",
    "        transitions[('cold','off')]  = [(0.7,'cold'), (0.3,'comfortable')]\n",
    "\n",
    "        # Comfortable state\n",
    "        transitions[('comfortable','heat')] = [(0.6,'comfortable'), (0.4,'hot')]\n",
    "        transitions[('comfortable','cool')] = [(0.6,'comfortable'), (0.4,'cold')]\n",
    "        transitions[('comfortable','off')]  = [(0.8,'comfortable'), (0.1,'cold'), (0.1,'hot')]\n",
    "\n",
    "        # Hot state\n",
    "        transitions[('hot','heat')] = [(0.2,'comfortable'), (0.8,'hot')]\n",
    "        transitions[('hot','cool')] = [(0.8,'comfortable'), (0.2,'hot')]\n",
    "        transitions[('hot','off')]  = [(0.3,'comfortable'), (0.7,'hot')]\n",
    "\n",
    "        # Rewards: comfortable=+2, energy cost for heat/cool=-0.5, extreme temps=-1\n",
    "        for s in states:\n",
    "            for a in ['heat','cool','off']:\n",
    "                for prob, ns in transitions[(s,a)]:\n",
    "                    r = 2.0 if ns == 'comfortable' else -1.0\n",
    "                    if a in ['heat','cool']:\n",
    "                        r -= 0.5  # energy cost\n",
    "                    rewards[(s, a, ns)] = r\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "thermo = ThermostatMDP(gamma=0.9)\n",
    "thermo.print_summary()\n",
    "\n",
    "for s in ['cold', 'comfortable', 'hot']:\n",
    "    for a in ['heat', 'cool', 'off']:\n",
    "        thermo.show_transitions_table(s, a)\n",
    "\n",
    "print(\"\\nğŸ’¡ Key tradeoff: heating when comfortable risks overheating, but 'off' slowly drifts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ° Environment 5: Multi-Armed Bandit (Contextual)\n",
    "\n",
    "A simplified slot machine problem framed as an MDP â€” the agent picks which machine to play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditMDP(MDP):\n",
    "    \"\"\"3-Armed Contextual Bandit as MDP.\n",
    "    States: morning, afternoon, evening (context)\n",
    "    Actions: machine_A, machine_B, machine_C\n",
    "    Each machine has different win rates depending on time of day.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9):\n",
    "        states = ['morning', 'afternoon', 'evening']\n",
    "        actions_map = {s: ['machine_A', 'machine_B', 'machine_C'] for s in states}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        # Win probabilities: (state, action) -> P(win)\n",
    "        win_probs = {\n",
    "            ('morning', 'machine_A'): 0.7, ('morning', 'machine_B'): 0.3, ('morning', 'machine_C'): 0.5,\n",
    "            ('afternoon','machine_A'): 0.4, ('afternoon','machine_B'): 0.6, ('afternoon','machine_C'): 0.5,\n",
    "            ('evening',  'machine_A'): 0.2, ('evening',  'machine_B'): 0.5, ('evening',  'machine_C'): 0.8,\n",
    "        }\n",
    "\n",
    "        # Time transitions (cyclic)\n",
    "        next_time = {'morning': 'afternoon', 'afternoon': 'evening', 'evening': 'morning'}\n",
    "\n",
    "        for s in states:\n",
    "            ns = next_time[s]\n",
    "            for a in actions_map[s]:\n",
    "                pw = win_probs[(s, a)]\n",
    "                transitions[(s, a)] = [(1.0, ns)]  # always move to next time\n",
    "                rewards[(s, a, ns)] = pw * 10 - (1-pw) * 2  # expected: win=+10, lose=-2\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "bandit = BanditMDP(gamma=0.9)\n",
    "bandit.print_summary()\n",
    "\n",
    "print(\"\\nğŸ° Expected rewards per (state, action):\")\n",
    "print(\"â•\" * 55)\n",
    "rows = []\n",
    "for s in ['morning', 'afternoon', 'evening']:\n",
    "    row = {'Time': s}\n",
    "    for a in ['machine_A', 'machine_B', 'machine_C']:\n",
    "        ts = bandit.get_transitions(s, a)\n",
    "        row[a] = f\"{ts[0][2]:.1f}\"\n",
    "    rows.append(row)\n",
    "display(pd.DataFrame(rows).set_index('Time'))\n",
    "print(\"\\nğŸ’¡ Optimal: A in morning, B in afternoon, C in evening!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ­ Environment 6: Inventory Management\n",
    "\n",
    "A store manages inventory. States are stock levels, actions are order quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InventoryMDP(MDP):\n",
    "    \"\"\"Inventory Management.\n",
    "    States: stock level (0, 1, 2, 3, 4)\n",
    "    Actions: order_0, order_1, order_2\n",
    "    Demand: stochastic (0, 1, or 2 items sold per period)\n",
    "    Costs: holding cost + stockout penalty + order cost\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9):\n",
    "        max_stock = 4\n",
    "        states = list(range(max_stock + 1))  # 0 to 4\n",
    "        actions_map = {s: ['order_0', 'order_1', 'order_2'] for s in states}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        demand_probs = {0: 0.3, 1: 0.5, 2: 0.2}  # P(demand = d)\n",
    "        order_cost = 1.0   # per unit ordered\n",
    "        hold_cost = 0.5    # per unit in stock\n",
    "        sale_revenue = 3.0 # per unit sold\n",
    "        stockout_penalty = -4.0  # per unit of unmet demand\n",
    "\n",
    "        for stock in states:\n",
    "            for action in ['order_0', 'order_1', 'order_2']:\n",
    "                order_qty = int(action.split('_')[1])\n",
    "                after_order = min(stock + order_qty, max_stock)\n",
    "                o_cost = order_qty * order_cost\n",
    "\n",
    "                trans = []\n",
    "                for demand, dp in demand_probs.items():\n",
    "                    sold = min(demand, after_order)\n",
    "                    unmet = demand - sold\n",
    "                    new_stock = after_order - sold\n",
    "\n",
    "                    reward = sold * sale_revenue - o_cost - new_stock * hold_cost + unmet * stockout_penalty\n",
    "\n",
    "                    found = False\n",
    "                    for i, (p, ns) in enumerate(trans):\n",
    "                        if ns == new_stock:\n",
    "                            trans[i] = (p + dp, ns)\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        trans.append((dp, new_stock))\n",
    "\n",
    "                    rewards[(stock, action, new_stock)] = reward\n",
    "\n",
    "                transitions[(stock, action)] = trans\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "inv = InventoryMDP(gamma=0.9)\n",
    "inv.print_summary()\n",
    "\n",
    "# Show what happens at stock=0 (empty!)\n",
    "for a in ['order_0', 'order_1', 'order_2']:\n",
    "    inv.show_transitions_table(0, a)\n",
    "print(\"\\nğŸ’¡ At stock=0, not ordering â†’ guaranteed stockout penalty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– Environment 7: Robot Navigation (Rooms)\n",
    "\n",
    "A robot navigates between rooms through doors. Some doors may be locked (stochastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotRoomsMDP(MDP):\n",
    "    \"\"\"Robot navigating 4 rooms with doors.\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Rm A â”‚ Rm B â”‚\n",
    "    â”‚      D      â”‚\n",
    "    â”œâ”€â”€Dâ”€â”€â”€â”¼â”€â”€Dâ”€â”€â”€â”¤\n",
    "    â”‚ Rm C â”‚ Rm D â”‚\n",
    "    â”‚  âš¡  â”‚  ğŸ†  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n",
    "    Rm C has charging station (small reward)\n",
    "    Rm D is the goal (big reward)\n",
    "    Doors may be locked with some probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9, lock_prob=0.2):\n",
    "        states = ['room_A', 'room_B', 'room_C', 'room_D']\n",
    "        # Adjacency: which rooms are connected\n",
    "        neighbors = {\n",
    "            'room_A': ['room_B', 'room_C'],\n",
    "            'room_B': ['room_A', 'room_D'],\n",
    "            'room_C': ['room_A', 'room_D'],\n",
    "            'room_D': ['room_B', 'room_C'],\n",
    "        }\n",
    "\n",
    "        actions_map = {}\n",
    "        transitions = {}\n",
    "        rewards = {}\n",
    "\n",
    "        for s in states:\n",
    "            acts = [f'go_{n}' for n in neighbors[s]] + ['stay']\n",
    "            actions_map[s] = acts\n",
    "\n",
    "            for a in acts:\n",
    "                if a == 'stay':\n",
    "                    transitions[(s, a)] = [(1.0, s)]\n",
    "                    rewards[(s, a, s)] = -0.1\n",
    "                    if s == 'room_C':\n",
    "                        rewards[(s, a, s)] = 1.0  # charging\n",
    "                else:\n",
    "                    target = a.replace('go_', '')\n",
    "                    transitions[(s, a)] = [\n",
    "                        (1 - lock_prob, target),  # door open\n",
    "                        (lock_prob, s)            # door locked\n",
    "                    ]\n",
    "                    if target == 'room_D':\n",
    "                        rewards[(s, a, target)] = 10.0  # goal!\n",
    "                    else:\n",
    "                        rewards[(s, a, target)] = -0.5  # movement cost\n",
    "                    rewards[(s, a, s)] = -0.5  # failed attempt\n",
    "\n",
    "        super().__init__(states, actions_map, transitions, rewards, gamma)\n",
    "\n",
    "\n",
    "robot = RobotRoomsMDP(gamma=0.9, lock_prob=0.2)\n",
    "robot.print_summary()\n",
    "\n",
    "robot.show_transitions_table('room_A', 'go_room_B')\n",
    "robot.show_transitions_table('room_B', 'go_room_D')\n",
    "print(\"\\nğŸ’¡ 20% chance any door is locked â†’ agent must plan for failure!\")\n",
    "print(\"   Should it go Aâ†’Bâ†’D or Aâ†’Câ†’D? Depends on lock probability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âš¡ Computing V(s) and Q(s,a)\n",
    "\n",
    "Now let's compute **value functions** for all our environments using the Bellman equation:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "$$Q^{\\pi}(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, theta=1e-8, max_iter=1000):\n",
    "    \"\"\"Compute V^pi for a given policy.\n",
    "    policy: dict {state: action}\n",
    "    Returns: dict {state: value}\n",
    "    \"\"\"\n",
    "    V = {s: 0.0 for s in mdp.states}\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            if s not in policy or not mdp.actions.get(s):\n",
    "                continue\n",
    "            a = policy[s]\n",
    "            new_v = 0\n",
    "            for prob, ns, reward in mdp.get_transitions(s, a):\n",
    "                new_v += prob * (reward + mdp.gamma * V[ns])\n",
    "            delta = max(delta, abs(V[s] - new_v))\n",
    "            V[s] = new_v\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def compute_q_values(mdp, V):\n",
    "    \"\"\"Compute Q(s,a) for all state-action pairs.\"\"\"\n",
    "    Q = {}\n",
    "    for s in mdp.states:\n",
    "        for a in mdp.actions.get(s, []):\n",
    "            q = 0\n",
    "            for prob, ns, reward in mdp.get_transitions(s, a):\n",
    "                q += prob * (reward + mdp.gamma * V[ns])\n",
    "            Q[(s, a)] = q\n",
    "    return Q\n",
    "\n",
    "\n",
    "def policy_improvement(mdp, V):\n",
    "    \"\"\"Compute greedy policy from V.\"\"\"\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        if not mdp.actions.get(s):\n",
    "            continue\n",
    "        best_a = None\n",
    "        best_v = -float('inf')\n",
    "        for a in mdp.actions[s]:\n",
    "            q = 0\n",
    "            for prob, ns, reward in mdp.get_transitions(s, a):\n",
    "                q += prob * (reward + mdp.gamma * V[ns])\n",
    "            if q > best_v:\n",
    "                best_v = q\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_iteration(mdp, max_iter=100):\n",
    "    \"\"\"Full policy iteration â†’ returns optimal V* and Ï€*.\"\"\"\n",
    "    # Start with arbitrary policy\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        if mdp.actions.get(s):\n",
    "            policy[s] = mdp.actions[s][0]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        V = policy_evaluation(mdp, policy)\n",
    "        new_policy = policy_improvement(mdp, V)\n",
    "        if new_policy == policy:\n",
    "            print(f\"  âœ… Policy converged in {i+1} iterations!\")\n",
    "            break\n",
    "        policy = new_policy\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "\n",
    "print(\"âœ… Value function and policy iteration functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Solve All Environments!\n",
    "\n",
    "Let's find the optimal policy and value function for each environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GRIDWORLD ===\n",
    "print(\"ğŸŒ GridWorld 5Ã—5 (Deterministic)\")\n",
    "gw_det = GridWorld(gamma=0.9, slip=0.0)\n",
    "V_gw, pi_gw = policy_iteration(gw_det)\n",
    "gw_det.visualize(values=V_gw, policy=pi_gw, title=\"GridWorld â€” Optimal Policy (Deterministic)\")\n",
    "\n",
    "print(\"\\nğŸŒ GridWorld 5Ã—5 (Stochastic, slip=0.2)\")\n",
    "gw_sto = GridWorld(gamma=0.9, slip=0.2)\n",
    "V_gws, pi_gws = policy_iteration(gw_sto)\n",
    "gw_sto.visualize(values=V_gws, policy=pi_gws, title=\"GridWorld â€” Optimal Policy (Stochastic)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Compare: stochastic policy avoids cells near the pit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FROZENLAKE ===\n",
    "print(\"ğŸ§Š FrozenLake 4Ã—4\")\n",
    "fl = FrozenLakeMDP(gamma=0.95, is_slippery=True)\n",
    "V_fl, pi_fl = policy_iteration(fl)\n",
    "fl.visualize(values=V_fl, policy=pi_fl, title=\"FrozenLake â€” Optimal Policy (Slippery)\")\n",
    "\n",
    "# Q-values for start state\n",
    "Q_fl = compute_q_values(fl, V_fl)\n",
    "print(\"\\nQ-values at START (0,0):\")\n",
    "for a in range(4):\n",
    "    q = Q_fl.get(((0,0), a), 0)\n",
    "    star = \" â˜… BEST\" if q == max(Q_fl.get(((0,0),i),0) for i in range(4)) else \"\"\n",
    "    print(f\"  {FrozenLakeMDP.ACTION_NAMES[a]} Q = {q:.4f}{star}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAFFIC LIGHT ===\n",
    "print(\"ğŸš¦ Traffic Light Controller\")\n",
    "traffic = TrafficLightMDP(gamma=0.9)\n",
    "V_tr, pi_tr = policy_iteration(traffic)\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s in sorted(V_tr.keys(), key=str):\n",
    "    print(f\"  {str(s):35s} â†’ {pi_tr.get(s, 'â€”'):10s}  V={V_tr[s]:.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ When traffic is high â†’ switch! When low â†’ keep current phase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === THERMOSTAT ===\n",
    "print(\"ğŸŒ¡ï¸ Thermostat Controller\")\n",
    "thermo = ThermostatMDP(gamma=0.9)\n",
    "V_th, pi_th = policy_iteration(thermo)\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s in ['cold', 'comfortable', 'hot']:\n",
    "    print(f\"  {s:15s} â†’ {pi_th[s]:8s}  V={V_th[s]:.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Coldâ†’heat, Hotâ†’cool, Comfortableâ†’off (save energy!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ALL Q-VALUES HEATMAPS ===\n",
    "print(\"ğŸ“Š Q-Value Heatmaps\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Traffic Q-values\n",
    "traffic_states = sorted(set(s for (s,a) in compute_q_values(traffic, V_tr).keys()), key=str)\n",
    "traffic_actions = ['keep', 'switch']\n",
    "q_matrix = np.zeros((len(traffic_states), len(traffic_actions)))\n",
    "Q_tr = compute_q_values(traffic, V_tr)\n",
    "for i, s in enumerate(traffic_states):\n",
    "    for j, a in enumerate(traffic_actions):\n",
    "        q_matrix[i,j] = Q_tr.get((s,a), 0)\n",
    "sns.heatmap(q_matrix, ax=axes[0], annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "           xticklabels=traffic_actions, yticklabels=[str(s) for s in traffic_states])\n",
    "axes[0].set_title('Traffic Light Q(s,a)', color='#22d3ee', fontweight='bold')\n",
    "\n",
    "# Thermostat Q-values\n",
    "thermo_states = ['cold', 'comfortable', 'hot']\n",
    "thermo_actions = ['heat', 'cool', 'off']\n",
    "q_matrix2 = np.zeros((3, 3))\n",
    "Q_th = compute_q_values(thermo, V_th)\n",
    "for i, s in enumerate(thermo_states):\n",
    "    for j, a in enumerate(thermo_actions):\n",
    "        q_matrix2[i,j] = Q_th.get((s,a), 0)\n",
    "sns.heatmap(q_matrix2, ax=axes[1], annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "           xticklabels=thermo_actions, yticklabels=thermo_states)\n",
    "axes[1].set_title('Thermostat Q(s,a)', color='#22d3ee', fontweight='bold')\n",
    "\n",
    "# Bandit Q-values\n",
    "bandit = BanditMDP(gamma=0.9)\n",
    "V_b, pi_b = policy_iteration(bandit)\n",
    "Q_b = compute_q_values(bandit, V_b)\n",
    "b_states = ['morning', 'afternoon', 'evening']\n",
    "b_actions = ['machine_A', 'machine_B', 'machine_C']\n",
    "q_matrix3 = np.zeros((3, 3))\n",
    "for i, s in enumerate(b_states):\n",
    "    for j, a in enumerate(b_actions):\n",
    "        q_matrix3[i,j] = Q_b.get((s,a), 0)\n",
    "sns.heatmap(q_matrix3, ax=axes[2], annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "           xticklabels=['A','B','C'], yticklabels=b_states)\n",
    "axes[2].set_title('Bandit Q(s,a)', color='#22d3ee', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nğŸ’¡ Brightest cell in each row = optimal action for that state!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¬ Experiment: Effect of Î³ (Discount Factor)\n",
    "\n",
    "How does the discount factor change the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.1, 0.5, 0.9, 0.99]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4.5))\n",
    "\n",
    "for idx, g in enumerate(gammas):\n",
    "    gw = GridWorld(gamma=g, slip=0.0)\n",
    "    V, pi = policy_iteration(gw)\n",
    "\n",
    "    ax = axes[idx]\n",
    "    ax.set_xlim(-0.5, 4.5); ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_aspect('equal'); ax.invert_yaxis()\n",
    "    arrow_map = {0:(0,-0.3), 1:(0.3,0), 2:(0,0.3), 3:(-0.3,0)}\n",
    "\n",
    "    all_v = [V[s] for s in V if s not in gw.walls and not (s==gw.goal or s==gw.pit)]\n",
    "    mn, mx = min(all_v), max(all_v)\n",
    "\n",
    "    for r in range(5):\n",
    "        for c in range(5):\n",
    "            if (r,c) in gw.walls:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#334155'))\n",
    "                ax.text(c,r,'ğŸ§±',ha='center',va='center',fontsize=14)\n",
    "            elif (r,c)==gw.goal:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#064e3b',alpha=0.5))\n",
    "                ax.text(c,r,'ğŸ†',ha='center',va='center',fontsize=14)\n",
    "            elif (r,c)==gw.pit:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color='#450a0a',alpha=0.5))\n",
    "                ax.text(c,r,'ğŸ•³ï¸',ha='center',va='center',fontsize=14)\n",
    "            else:\n",
    "                v = V.get((r,c), 0)\n",
    "                t = (v-mn)/(mx-mn+1e-8)\n",
    "                ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,color=plt.cm.RdYlGn(t),alpha=0.4))\n",
    "                ax.text(c,r+0.3,f\"{v:.1f}\",ha='center',va='center',fontsize=7,color='white')\n",
    "                if (r,c) in pi:\n",
    "                    a = pi[(r,c)]\n",
    "                    dy,dx = arrow_map[a]\n",
    "                    ax.annotate('',xy=(c+dx,r+dy),xytext=(c,r),\n",
    "                               arrowprops=dict(arrowstyle='->',color='#22d3ee',lw=2))\n",
    "            ax.add_patch(plt.Rectangle((c-0.5,r-0.5),1,1,fill=False,edgecolor='#334155',lw=0.3))\n",
    "\n",
    "    ax.set_title(f'Î³ = {g}', fontsize=12, fontweight='bold', color='#22d3ee')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for s in ax.spines.values(): s.set_visible(False)\n",
    "\n",
    "plt.suptitle('Effect of Discount Factor Î³ on Optimal Policy', fontsize=14,\n",
    "            fontweight='bold', color='white', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Î³=0.1: short-sighted â€” only cells very near the goal have high value\")\n",
    "print(\"   Î³=0.99: far-sighted â€” the goal's value propagates across the entire grid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¬ Experiment: Deterministic vs Stochastic\n",
    "\n",
    "Compare policies side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deterministic (slip=0) vs Stochastic (slip=0.3)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "gw_d = GridWorld(gamma=0.9, slip=0.0)\n",
    "V_d, pi_d = policy_iteration(gw_d)\n",
    "\n",
    "gw_s = GridWorld(gamma=0.9, slip=0.3)\n",
    "V_s, pi_s = policy_iteration(gw_s)\n",
    "\n",
    "gw_d.visualize(values=V_d, policy=pi_d, title=\"Deterministic (slip=0)\")\n",
    "gw_s.visualize(values=V_s, policy=pi_s, title=\"Stochastic (slip=0.3)\")\n",
    "\n",
    "# Show differences\n",
    "print(\"\\nPolicy differences (cells where arrows differ):\")\n",
    "arrow_names = {0: 'â†', 1: 'â†“', 2: 'â†’', 3: 'â†‘'}\n",
    "for s in sorted(pi_d.keys()):\n",
    "    if s in pi_s and pi_d[s] != pi_s[s]:\n",
    "        print(f\"  State {s}: deterministic={arrow_names[pi_d[s]]}  stochastic={arrow_names[pi_s[s]]}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ The stochastic policy takes wider detours to avoid accidentally falling in the pit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### What We Built\n",
    "| # | Environment | States | Actions | Key Lesson |\n",
    "|---|------------|--------|---------|------------|\n",
    "| 1 | GridWorld 5Ã—5 | 22 cells | â†â†“â†’â†‘ | Spatial navigation, walls, rewards |\n",
    "| 2 | FrozenLake 4Ã—4 | 16 cells | â†â†“â†’â†‘ | Slippery surfaces, holes |\n",
    "| 3 | Traffic Light | 6 states | keep/switch | Real-world control, stochastic demand |\n",
    "| 4 | Thermostat | 3 states | heat/cool/off | Energy vs comfort tradeoff |\n",
    "| 5 | Bandit | 3 contexts | 3 machines | Context-dependent rewards |\n",
    "| 6 | Inventory | 5 levels | order 0/1/2 | Supply chain, stochastic demand |\n",
    "| 7 | Robot Rooms | 4 rooms | go/stay | Door uncertainty, path planning |\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Every environment is an MDP** â€” states, actions, transitions, rewards, discount\n",
    "2. **Deterministic vs Stochastic** â€” stochastic environments need cautious policies\n",
    "3. **Î³ controls planning horizon** â€” low Î³ = greedy, high Î³ = far-sighted\n",
    "4. **Q(s,a)** tells you exactly how good each action is in each state\n",
    "5. **Policy Iteration** finds the optimal policy for any MDP\n",
    "\n",
    "---\n",
    "ğŸ“˜ **Next:** Chapter 3 â€” Estimating Value Functions with Dynamic Programming  \n",
    "ğŸ”— [Policy Iteration Interactive Lab](https://mlnjsh.github.io/rl-book-labs/ch3/)"
   ]
  }
 ]
}
